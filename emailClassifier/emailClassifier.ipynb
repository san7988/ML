{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\sasnjeev\\anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Import done...\n"
     ]
    }
   ],
   "source": [
    "print \"Importing...\"\n",
    "import os\n",
    "import io\n",
    "from pandas import DataFrame\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print \"Import done...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading mails...\n",
      "Reading D:\\sasnjeev\\enron_data\\beck-s\n",
      "Reading D:\\sasnjeev\\enron_data\\farmer-d\n",
      "Reading D:\\sasnjeev\\enron_data\\kaminski-v\n",
      "Reading D:\\sasnjeev\\enron_data\\kitchen-l\n",
      "Reading D:\\sasnjeev\\enron_data\\lokay-m\n",
      "Reading D:\\sasnjeev\\enron_data\\williams-w3\n",
      "Reading D:\\sasnjeev\\enron_data\\BG\n",
      "Reading D:\\sasnjeev\\enron_data\\GP\n",
      "Reading D:\\sasnjeev\\enron_data\\SH\n",
      "Reindexing done. Dataframe ready...\n"
     ]
    }
   ],
   "source": [
    "NEWLINE = '\\n'\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = io.open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n",
    "\n",
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    read_all = False\n",
    "    mails_to_read = 500\n",
    "    count = 1\n",
    "    for file_name, text in read_files(path):\n",
    "        if not read_all:\n",
    "            if count <= mails_to_read:\n",
    "                count += 1\n",
    "                rows.append({'text': text, 'class': classification})\n",
    "                index.append(file_name)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "#             print \"Reading all...\"\n",
    "            rows.append({'text': text, 'class': classification})\n",
    "            index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame\n",
    "\n",
    "DATA_PATH = \"D:\\\\sasnjeev\\\\enron_data\\\\\"\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "#Explicit labelling of folder, just to make life easy\n",
    "SOURCES = [\n",
    "    ('beck-s', HAM),\n",
    "    ('farmer-d', HAM),\n",
    "    ('kaminski-v', HAM),\n",
    "    ('kitchen-l', HAM),\n",
    "    ('lokay-m', HAM),\n",
    "    ('williams-w3', HAM),\n",
    "    ('BG', SPAM),\n",
    "    ('GP', SPAM),\n",
    "    ('SH', SPAM)\n",
    "]\n",
    "print \"Reading mails...\"\n",
    "data = DataFrame({'text': [], 'class': []})\n",
    "\n",
    "for path, classification in SOURCES:\n",
    "    print \"Reading \" +DATA_PATH+path\n",
    "    data = data.append(build_data_frame(DATA_PATH+path, classification))\n",
    "    \n",
    "data = data.reindex(numpy.random.permutation(data.index))\n",
    "print \"Reindexing done. Dataframe ready...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Predicting...\n",
      "Fitting...\n",
      "Predicting...\n",
      "Fitting...\n",
      "Predicting...\n",
      "Fitting...\n",
      "Predicting...\n",
      "Fitting...\n",
      "Predicting...\n",
      "Fitting...\n",
      "Predicting...\n",
      "Total emails classified: 4500\n",
      "Score: 0.965165074532\n",
      "Confusion matrix:\n",
      "[[2988   12]\n",
      " [  90 1410]]\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "#     print len(train_indices), len(test_indices)\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    print \"Fitting...\"\n",
    "    counts = count_vectorizer.fit_transform(train_text)\n",
    "#     print counts\n",
    "#     classifier = MultinomialNB()\n",
    "#     targets = data['class'].values\n",
    "    classifier = RandomForestClassifier()\n",
    "    classifier.fit(counts, train_y)\n",
    "    print \"Predicting...\"\n",
    "    test_count = count_vectorizer.transform(test_text)\n",
    "    pred = classifier.predict(test_count)\n",
    "    confusion += confusion_matrix(test_y, pred)\n",
    "    score = f1_score(test_y, pred, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "print 'Total emails classified: '+ str(len(data))\n",
    "print 'Score: ' + str(sum(scores)/len(scores))\n",
    "print 'Confusion matrix:'\n",
    "print confusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
